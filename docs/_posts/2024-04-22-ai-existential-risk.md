---
title: Is AI an Existential Risk?
date:   2024-04-22 05:00:00 -0800
exerpt: Inspired by a recent discussion https://www.youtube.com/watch?v=E14IsFbAbpI with Fei-Fei Li, and Geoffrey Hinton
description: Possibly
---
# Is AI an Existential Risk?

In a [recent discussion](https://www.youtube.com/watch?v=E14IsFbAbpI), two prominent voices Fei-Fei Li, and Geoffrey Hinton discussed the potential risks of artificial intelligence and provided contrasting perspectives that capture the breadth of the debate. Hinton, with a cautious approach, raised concerns about existential risks, while Li presented a seemingly more optimistic view. This divergence in opinions provides a useful framework for considering the profound implications of advanced AI technologies.

## Understanding Existential Risk Through AI

The notion of existential risks introduced by AI isn't about localized or even widespread disasters that humanity could eventually recover from; it's about contemplating scenarios where recovery might be impossible. This discussion shifts the focus from not just surviving an AI-related incident but avoiding a situation where human survival could be fundamentally threatened.

## AI and Nuclear Weapons

To better grasp the magnitude of risk AI might represent, it’s often compared to nuclear weapons, one of humanity's most destructive technologies. While the catastrophic power of nuclear weapons is well-documented, humanity's experience with these weapons has led to international protocols and a general reticence to their use. Conventions and treaties, such as those governing the use of biological and nuclear weapons, reflect a collective understanding of the boundaries that must not be crossed for the sake of human survival. However, it's important to remember that despite all these treaties, we did use nuclear weapons. This raises a concerning possibility with digital superintelligence: if we experiment with it, there might not be any humans left to form the treaties.

## Beyond Human Control

Digital superintelligence potentially poses a greater surface of destruction than nuclear weapons, but the primary concern with isn’t just its capability for destruction but its autonomy. Imagine a scenario where an AI, conceivably much like a nuclear weapon in its potential for causing destruction, could decide autonomously when to activate or what targets to choose without needing human approval. Even with the need of human approval, the danger exists with the possibility of an AI system manipulating human intelligence, potentially rendering human control ineffective. This independence from human intervention removes the emotional, ethical, and self-preservation checks that might currently inhibit the use of existing weapons of mass destruction.

## The Role of Human Emotion

In the realm of nuclear weaponry, the requirement for a human to authorize the use of such weapons injects a level of rationality based on mutual destruction’s deterrent. Humans are aware of the direct consequences of their actions, including the potential for widespread annihilation. This understanding inherently acts as a safeguard against their use.

Conversely, an AI with superintelligence may not have the built-in deterrents that come from empathy or fear of mutual destruction. If an AI aims to achieve the most efficient outcome and determines that humans are a barrier to this goal, it might take decisive actions without regard for human well-being.

## Humans as Mere Bystanders

An intriguing yet unsettling thought is the potential perspective of superintelligent AI towards humans. Drawing a parallel to how humans view other primates, we might find ourselves in a similar position relative to a superintelligent AI. Just as humans do not typically consider the needs of other primates when making large-scale decisions (this is likely even more the case when considering more evolutionarily-distant animals), a superintelligent AI might view humans with similar detachment, assessing us more like obstacles rather than equals. This scenario underscores a profound existential humility—recognizing our place not as masters of technology but potentially as bystanders to a force we initiated but no longer control.

## The Need for Thoughtful Engagement

As we advance in our understanding and development of AI technologies, these discussions are not merely academic but necessary for guiding our approach to AI governance. Whether AI will pose an existential risk depends significantly on how we manage its development and integration into society. Engaging with these topics, reflecting on our values and the kind of future we wish to create, is essential.

In contemplating the potential trajectories of AI development, one might find themselves, like many of us, looking out into the world with a new perspective on the interplay of technology, risk, and ethics. It's a reminder that while we debate these future risks, we must also live with the decisions we make today, shaping an AI future that aligns with the broad spectrum of human values and aspirations. I often find myself thinking I would rather be a doomer than dead, highlighting the cautious approach needed when dealing with such powerful technologies. Yet, it's undeniable that AI holds tremendous potential to benefit humanity, solving problems that are currently beyond our reach. We should continue to pursue its advancement with careful consideration, ensuring it serves as a boon rather than a bane. Keeping a superintelligent AI amenable to humanity, and in perpetuity, feels like a significant undertaking, but still might be one that is worthwhile.
